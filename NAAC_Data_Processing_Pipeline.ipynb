{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "171550c0",
   "metadata": {},
   "source": [
    "# üéì NAAC Data Processing Pipeline\n",
    "## Comprehensive AI Assistant Data Preparation\n",
    "\n",
    "This notebook implements the complete pipeline for processing NAAC datasets and documents to create an intelligent AI assistant capable of:\n",
    "\n",
    "- üìä **Data Analysis**: Processing institutional data from Kaggle and Data.gov\n",
    "- üìÑ **Document Processing**: Extracting and chunking text from SSRs, AQARs, and guidelines\n",
    "- üß† **Vector Search**: Creating embeddings for semantic similarity search\n",
    "- ü§ñ **RAG Implementation**: Building Retrieval-Augmented Generation pipeline with IBM Granite LLM\n",
    "- ‚úÖ **Interactive Testing**: Validating the AI assistant's capabilities\n",
    "\n",
    "---\n",
    "\n",
    "### üìã **Pipeline Overview**\n",
    "\n",
    "1. **Setup Environment** - Install dependencies and configure paths\n",
    "2. **Organize Files** - Create structured directories for data management\n",
    "3. **Clean Tabular Data** - Process CSV/Excel files for metadata enrichment\n",
    "4. **Extract PDF Text** - Convert NAAC documents to searchable text\n",
    "5. **Chunk for RAG** - Split text into optimal retrieval segments\n",
    "6. **Create Vector DB** - Generate embeddings and build search index\n",
    "7. **Build RAG Pipeline** - Connect retrieval with IBM Granite LLM\n",
    "8. **Test Assistant** - Validate performance with real NAAC queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af2bda9",
   "metadata": {},
   "source": [
    "## üîß Section 1: Setup Environment and Import Libraries\n",
    "\n",
    "First, we'll install and import all necessary libraries for data processing, text extraction, and RAG implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a096768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pandas openpyxl pdfplumber PyMuPDF langchain chromadb sentence-transformers\n",
    "!pip install langchain-community langchain-text-splitters\n",
    "!pip install huggingface-hub transformers torch\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4d32db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdfplumber\n",
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "# Set up paths\n",
    "BASE_DIR = Path(\"/home/hari/naac\")\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "CLEANED_DIR = DATA_DIR / \"cleaned\"\n",
    "DOCS_DIR = DATA_DIR / \"documents\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "print(\"üìö Libraries imported successfully!\")\n",
    "print(f\"üìÅ Base directory: {BASE_DIR}\")\n",
    "print(f\"üìä Data directory: {DATA_DIR}\")\n",
    "print(f\"üìã Raw data: {RAW_DIR}\")\n",
    "print(f\"‚ú® Cleaned data: {CLEANED_DIR}\")\n",
    "print(f\"üìÑ Documents: {DOCS_DIR}\")\n",
    "print(f\"‚öôÔ∏è Processed: {PROCESSED_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92cefd6",
   "metadata": {},
   "source": [
    "## üìÅ Section 2: Organize File Structure\n",
    "\n",
    "Create the proper directory structure for organized data management as per the step-by-step plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bfec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all directories exist\n",
    "directories = [DATA_DIR, RAW_DIR, CLEANED_DIR, DOCS_DIR, PROCESSED_DIR]\n",
    "for directory in directories:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"‚úÖ Directory created/verified: {directory}\")\n",
    "\n",
    "# Check existing raw data files\n",
    "print(\"\\nüìã Existing raw data files:\")\n",
    "raw_files = list(RAW_DIR.glob(\"*\"))\n",
    "for i, file in enumerate(raw_files, 1):\n",
    "    print(f\"  {i}. {file.name} ({file.stat().st_size / 1024:.1f} KB)\")\n",
    "\n",
    "if not raw_files:\n",
    "    print(\"  ‚ö†Ô∏è  No files found in raw data directory\")\n",
    "    print(\"  üí° Please ensure your datasets are in:\", RAW_DIR)\n",
    "else:\n",
    "    print(f\"\\nüéâ Found {len(raw_files)} data files ready for processing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087e3e6b",
   "metadata": {},
   "source": [
    "## üîç Section 3: Load and Clean Tabular Data\n",
    "\n",
    "Process CSV and Excel files from Kaggle and Data.gov to create clean, structured datasets for metadata enrichment and query filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736f8d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df, filename):\n",
    "    \"\"\"Clean and standardize a dataframe\"\"\"\n",
    "    print(f\"\\nüßπ Cleaning {filename}...\")\n",
    "    print(f\"   Original shape: {df.shape}\")\n",
    "    \n",
    "    # Store original shape\n",
    "    original_rows = len(df)\n",
    "    \n",
    "    # Clean column names\n",
    "    df.columns = df.columns.astype(str).str.strip().str.lower().str.replace(' ', '_')\n",
    "    \n",
    "    # Remove completely empty rows\n",
    "    df = df.dropna(how='all')\n",
    "    \n",
    "    # Remove columns that are completely empty\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    \n",
    "    print(f\"   After cleaning: {df.shape}\")\n",
    "    print(f\"   Removed {original_rows - len(df)} empty rows\")\n",
    "    print(f\"   Columns: {list(df.columns[:5])}{'...' if len(df.columns) > 5 else ''}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Process CSV files\n",
    "csv_files = list(RAW_DIR.glob(\"*.csv\"))\n",
    "cleaned_datasets = {}\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    try:\n",
    "        # Load CSV\n",
    "        df = pd.read_csv(csv_file, encoding='utf-8')\n",
    "        \n",
    "        # Clean the dataframe\n",
    "        df_clean = clean_dataframe(df, csv_file.name)\n",
    "        \n",
    "        # Save cleaned version\n",
    "        cleaned_filename = f\"cleaned_{csv_file.stem}.csv\"\n",
    "        cleaned_path = CLEANED_DIR / cleaned_filename\n",
    "        df_clean.to_csv(cleaned_path, index=False)\n",
    "        \n",
    "        # Store in memory for later use\n",
    "        cleaned_datasets[csv_file.stem] = df_clean\n",
    "        \n",
    "        print(f\"   ‚úÖ Saved: {cleaned_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error processing {csv_file.name}: {e}\")\n",
    "\n",
    "print(f\"\\nüìä Successfully processed {len(cleaned_datasets)} CSV files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7650a0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Excel files\n",
    "excel_files = list(RAW_DIR.glob(\"*.xlsx\")) + list(RAW_DIR.glob(\"*.xls\"))\n",
    "\n",
    "for excel_file in excel_files:\n",
    "    try:\n",
    "        # Load Excel file (might have multiple sheets)\n",
    "        excel_data = pd.ExcelFile(excel_file)\n",
    "        print(f\"\\nüìä Processing {excel_file.name}\")\n",
    "        print(f\"   Sheets found: {excel_data.sheet_names}\")\n",
    "        \n",
    "        for sheet_name in excel_data.sheet_names:\n",
    "            # Read each sheet\n",
    "            df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "            \n",
    "            # Clean the dataframe\n",
    "            df_clean = clean_dataframe(df, f\"{excel_file.stem}_{sheet_name}\")\n",
    "            \n",
    "            # Save cleaned version\n",
    "            cleaned_filename = f\"cleaned_{excel_file.stem}_{sheet_name}.csv\"\n",
    "            cleaned_path = CLEANED_DIR / cleaned_filename\n",
    "            df_clean.to_csv(cleaned_path, index=False)\n",
    "            \n",
    "            # Store in memory\n",
    "            cleaned_datasets[f\"{excel_file.stem}_{sheet_name}\"] = df_clean\n",
    "            \n",
    "            print(f\"   ‚úÖ Saved: {cleaned_path}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error processing {excel_file.name}: {e}\")\n",
    "\n",
    "# Create summary of all datasets\n",
    "print(f\"\\nüìà Dataset Summary:\")\n",
    "print(f\"{'Dataset':<30} {'Rows':<8} {'Columns':<8} {'Size (KB)':<10}\")\n",
    "print(\"-\" * 56)\n",
    "\n",
    "total_rows = 0\n",
    "for name, df in cleaned_datasets.items():\n",
    "    size_kb = df.memory_usage(deep=True).sum() / 1024\n",
    "    total_rows += len(df)\n",
    "    print(f\"{name[:29]:<30} {len(df):<8} {len(df.columns):<8} {size_kb:<10.1f}\")\n",
    "\n",
    "print(\"-\" * 56)\n",
    "print(f\"{'TOTAL':<30} {total_rows:<8} {'':<8} {'':<10}\")\n",
    "print(f\"\\nüéâ All tabular data cleaned and ready for use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522c4f8c",
   "metadata": {},
   "source": [
    "## üìÑ Section 4: Extract Text from PDF Documents\n",
    "\n",
    "Extract text content from NAAC SSRs, AQARs, and guideline documents using pdfplumber for reliable text extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ba5aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from PDF using pdfplumber\"\"\"\n",
    "    text_content = \"\"\n",
    "    page_count = 0\n",
    "    \n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            page_count = len(pdf.pages)\n",
    "            print(f\"   üìñ Extracting from {page_count} pages...\")\n",
    "            \n",
    "            for page_num, page in enumerate(pdf.pages, 1):\n",
    "                try:\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text_content += f\"\\n--- Page {page_num} ---\\n\"\n",
    "                        text_content += page_text.strip()\n",
    "                        text_content += \"\\n\"\n",
    "                        \n",
    "                        if page_num % 10 == 0:  # Progress indicator\n",
    "                            print(f\"   ‚è≥ Processed {page_num}/{page_count} pages...\")\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Error on page {page_num}: {e}\")\n",
    "                    continue\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error opening PDF: {e}\")\n",
    "        return None, 0\n",
    "    \n",
    "    return text_content.strip(), page_count\n",
    "\n",
    "# Look for PDF files in documents directory and raw directory\n",
    "pdf_files = list(DOCS_DIR.glob(\"*.pdf\")) + list(RAW_DIR.glob(\"*.pdf\"))\n",
    "extracted_texts = {}\n",
    "\n",
    "if not pdf_files:\n",
    "    print(\"üìù No PDF files found. Creating sample placeholder...\")\n",
    "    \n",
    "    # Create a sample NAAC document for demonstration\n",
    "    sample_content = \"\"\"\n",
    "NATIONAL ASSESSMENT AND ACCREDITATION COUNCIL (NAAC)\n",
    "Self Study Report (SSR)\n",
    "\n",
    "1. EXECUTIVE SUMMARY\n",
    "\n",
    "1.1 INTRODUCTION\n",
    "The institution is committed to providing quality higher education and has established comprehensive mechanisms for quality assurance. This Self Study Report (SSR) presents a detailed analysis of the institution's performance across seven criteria.\n",
    "\n",
    "1.2 INSTITUTIONAL PROFILE\n",
    "Name of the Institution: Sample College\n",
    "Year of Establishment: 1985\n",
    "Type: Government/Government Aided/Self-financing\n",
    "Academic Programs: Undergraduate, Postgraduate, Research\n",
    "\n",
    "2. INSTITUTIONAL PREPAREDNESS FOR ACCREDITATION\n",
    "\n",
    "2.1 QUALITY POLICY\n",
    "The institution has a well-defined quality policy that emphasizes excellence in teaching, learning, and research.\n",
    "\n",
    "2.2 IQAC (Internal Quality Assurance Cell)\n",
    "The IQAC functions effectively with regular meetings and quality enhancement initiatives.\n",
    "\n",
    "3. CRITERIA-WISE ANALYSIS\n",
    "\n",
    "3.1 CRITERIA I: CURRICULAR ASPECTS\n",
    "The institution offers diverse programs aligned with university guidelines and industry requirements.\n",
    "\n",
    "3.2 CRITERIA II: TEACHING-LEARNING AND EVALUATION\n",
    "Innovative teaching methodologies and fair evaluation practices are implemented.\n",
    "\n",
    "3.3 CRITERIA III: RESEARCH, INNOVATIONS AND EXTENSION\n",
    "The institution encourages research activities and community engagement.\n",
    "\n",
    "3.4 CRITERIA IV: INFRASTRUCTURE AND LEARNING RESOURCES\n",
    "Adequate infrastructure and modern learning resources support academic activities.\n",
    "\n",
    "3.5 CRITERIA V: STUDENT SUPPORT AND PROGRESSION\n",
    "Comprehensive student support services ensure holistic development.\n",
    "\n",
    "3.6 CRITERIA VI: GOVERNANCE, LEADERSHIP AND MANAGEMENT\n",
    "Effective governance structures and leadership promote institutional growth.\n",
    "\n",
    "3.7 CRITERIA VII: INSTITUTIONAL VALUES AND BEST PRACTICES\n",
    "The institution upholds ethical values and implements innovative best practices.\n",
    "\n",
    "4. CONCLUSION\n",
    "The institution demonstrates commitment to quality education and continuous improvement.\n",
    "\"\"\"\n",
    "    \n",
    "    # Save sample document\n",
    "    sample_path = DOCS_DIR / \"sample_naac_ssr.txt\"\n",
    "    with open(sample_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(sample_content)\n",
    "    \n",
    "    extracted_texts['sample_naac_ssr'] = sample_content\n",
    "    print(f\"   ‚úÖ Created sample document: {sample_path}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"üìö Found {len(pdf_files)} PDF files to process:\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nüìÑ Processing: {pdf_file.name}\")\n",
    "        \n",
    "        # Extract text\n",
    "        text_content, page_count = extract_text_from_pdf(pdf_file)\n",
    "        \n",
    "        if text_content:\n",
    "            # Save extracted text\n",
    "            text_filename = f\"{pdf_file.stem}_extracted.txt\"\n",
    "            text_path = PROCESSED_DIR / text_filename\n",
    "            \n",
    "            with open(text_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(text_content)\n",
    "            \n",
    "            # Store in memory\n",
    "            extracted_texts[pdf_file.stem] = text_content\n",
    "            \n",
    "            word_count = len(text_content.split())\n",
    "            print(f\"   ‚úÖ Extracted {word_count:,} words from {page_count} pages\")\n",
    "            print(f\"   üíæ Saved to: {text_path}\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Failed to extract text from {pdf_file.name}\")\n",
    "\n",
    "print(f\"\\nüìñ Text extraction complete!\")\n",
    "print(f\"   üìö Total documents processed: {len(extracted_texts)}\")\n",
    "total_words = sum(len(text.split()) for text in extracted_texts.values())\n",
    "print(f\"   üìù Total words extracted: {total_words:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bb6a1b",
   "metadata": {},
   "source": [
    "## üß© Section 5: Chunk Text for RAG Implementation\n",
    "\n",
    "Break the extracted text into meaningful chunks using LangChain's RecursiveCharacterTextSplitter for optimal retrieval performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681dc59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure text splitter for optimal chunk size\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,        # Optimal size for semantic coherence\n",
    "    chunk_overlap=150,      # Overlap to maintain context\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Prioritize natural breaks\n",
    ")\n",
    "\n",
    "print(\"üß© Chunking text documents for RAG...\")\n",
    "print(f\"   Chunk size: 1000 characters\")\n",
    "print(f\"   Overlap: 150 characters\")\n",
    "\n",
    "all_chunks = []\n",
    "chunk_metadata = []\n",
    "\n",
    "for doc_name, text_content in extracted_texts.items():\n",
    "    print(f\"\\nüìÑ Chunking: {doc_name}\")\n",
    "    \n",
    "    # Split text into chunks\n",
    "    chunks = text_splitter.split_text(text_content)\n",
    "    \n",
    "    print(f\"   üìä Created {len(chunks)} chunks\")\n",
    "    \n",
    "    # Create Document objects with metadata\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Create document with metadata\n",
    "        doc = Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\n",
    "                \"source\": doc_name,\n",
    "                \"chunk_id\": f\"{doc_name}_chunk_{i+1}\",\n",
    "                \"chunk_index\": i,\n",
    "                \"total_chunks\": len(chunks),\n",
    "                \"doc_type\": \"naac_document\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        all_chunks.append(doc)\n",
    "        \n",
    "        # Store metadata for analysis\n",
    "        chunk_metadata.append({\n",
    "            \"source\": doc_name,\n",
    "            \"chunk_id\": f\"{doc_name}_chunk_{i+1}\",\n",
    "            \"chunk_size\": len(chunk),\n",
    "            \"chunk_index\": i\n",
    "        })\n",
    "    \n",
    "    # Show preview of first chunk\n",
    "    if chunks:\n",
    "        preview = chunks[0][:200] + \"...\" if len(chunks[0]) > 200 else chunks[0]\n",
    "        print(f\"   üìñ Preview: {preview}\")\n",
    "\n",
    "# Save chunks for future use\n",
    "chunks_path = PROCESSED_DIR / \"text_chunks.json\"\n",
    "chunks_data = {\n",
    "    \"chunks\": [{\"content\": doc.page_content, \"metadata\": doc.metadata} for doc in all_chunks],\n",
    "    \"total_chunks\": len(all_chunks),\n",
    "    \"processing_timestamp\": pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(chunks_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunks_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Text chunking complete!\")\n",
    "print(f\"   üìö Total chunks created: {len(all_chunks)}\")\n",
    "print(f\"   üìä Average chunk size: {np.mean([len(doc.page_content) for doc in all_chunks]):.0f} characters\")\n",
    "print(f\"   üíæ Chunks saved to: {chunks_path}\")\n",
    "\n",
    "# Display chunk distribution\n",
    "chunk_sizes = [len(doc.page_content) for doc in all_chunks]\n",
    "print(f\"   üìà Chunk size distribution:\")\n",
    "print(f\"      Min: {min(chunk_sizes)} characters\")\n",
    "print(f\"      Max: {max(chunk_sizes)} characters\")\n",
    "print(f\"      Median: {np.median(chunk_sizes):.0f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6896859",
   "metadata": {},
   "source": [
    "## üß† Section 6: Create Vector Embeddings and Database\n",
    "\n",
    "Generate embeddings using HuggingFace models and store them in ChromaDB for efficient similarity search and retrieval operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0034e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß† Setting up embeddings and vector database...\")\n",
    "\n",
    "# Initialize embedding model\n",
    "print(\"   üîß Loading HuggingFace embedding model...\")\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",  # Fast and efficient\n",
    "    model_kwargs={'device': 'cpu'},  # Use CPU for compatibility\n",
    "    encode_kwargs={'normalize_embeddings': True}  # Normalize for better similarity\n",
    ")\n",
    "\n",
    "print(\"   ‚úÖ Embedding model loaded successfully!\")\n",
    "\n",
    "# Set up ChromaDB path\n",
    "chroma_db_path = PROCESSED_DIR / \"chroma_db\"\n",
    "chroma_db_path.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"   üìÅ Vector database path: {chroma_db_path}\")\n",
    "\n",
    "# Create vector database\n",
    "print(\"   üóÑÔ∏è Creating vector database...\")\n",
    "print(f\"   üìä Processing {len(all_chunks)} text chunks...\")\n",
    "\n",
    "# Create ChromaDB vector store\n",
    "try:\n",
    "    vector_db = Chroma.from_documents(\n",
    "        documents=all_chunks,\n",
    "        embedding=embedding_model,\n",
    "        persist_directory=str(chroma_db_path),\n",
    "        collection_name=\"naac_documents\"\n",
    "    )\n",
    "    \n",
    "    # Persist the database\n",
    "    vector_db.persist()\n",
    "    \n",
    "    print(\"   ‚úÖ Vector database created successfully!\")\n",
    "    \n",
    "    # Test the database\n",
    "    print(\"\\nüîç Testing vector database...\")\n",
    "    test_query = \"NAAC accreditation criteria\"\n",
    "    test_results = vector_db.similarity_search(test_query, k=3)\n",
    "    \n",
    "    print(f\"   üîé Test query: '{test_query}'\")\n",
    "    print(f\"   üìä Retrieved {len(test_results)} similar chunks:\")\n",
    "    \n",
    "    for i, result in enumerate(test_results, 1):\n",
    "        preview = result.page_content[:100] + \"...\" if len(result.page_content) > 100 else result.page_content\n",
    "        source = result.metadata.get('source', 'Unknown')\n",
    "        print(f\"      {i}. Source: {source}\")\n",
    "        print(f\"         Preview: {preview}\")\n",
    "        print()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error creating vector database: {e}\")\n",
    "    print(\"   üí° This might be due to missing dependencies. Installing...\")\n",
    "    \n",
    "    # Try to install missing dependencies\n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"chromadb\", \"--quiet\"])\n",
    "        print(\"   ‚úÖ ChromaDB installed. Please restart and try again.\")\n",
    "    except:\n",
    "        print(\"   ‚ö†Ô∏è  Manual installation required: pip install chromadb\")\n",
    "\n",
    "# Save vector database metadata\n",
    "db_metadata = {\n",
    "    \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"total_chunks\": len(all_chunks),\n",
    "    \"database_path\": str(chroma_db_path),\n",
    "    \"collection_name\": \"naac_documents\",\n",
    "    \"creation_timestamp\": pd.Timestamp.now().isoformat(),\n",
    "    \"chunk_sources\": list(set([chunk.metadata['source'] for chunk in all_chunks]))\n",
    "}\n",
    "\n",
    "metadata_path = PROCESSED_DIR / \"vector_db_metadata.json\"\n",
    "with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(db_metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\nüéâ Vector database setup complete!\")\n",
    "print(f\"   üóÑÔ∏è Database location: {chroma_db_path}\")\n",
    "print(f\"   üìä Total vectors: {len(all_chunks)}\")\n",
    "print(f\"   üìù Metadata saved: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f74e4d3",
   "metadata": {},
   "source": [
    "## ü§ñ Section 7: Build RAG Pipeline with LangChain\n",
    "\n",
    "Implement RetrievalQA chain connecting the vector database with IBM Granite LLM for question-answering capabilities on NAAC documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ed56cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables for IBM Cloud\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"/home/hari/naac/naac-frontend/.env\")  # Load from your .env file\n",
    "\n",
    "class IBMGraniteLLM(LLM):\n",
    "    \"\"\"Custom LLM wrapper for IBM Granite model\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.api_key = os.getenv(\"REACT_APP_IBM_CLOUD_API_KEY\")\n",
    "        self.model_id = os.getenv(\"REACT_APP_IBM_GRANITE_MODEL_ID\", \"ibm/granite-13b-chat-v2\")\n",
    "        self.url = os.getenv(\"REACT_APP_IBM_GRANITE_URL\")\n",
    "        self.project_id = os.getenv(\"REACT_APP_IBM_PROJECT_ID\")\n",
    "        \n",
    "        if not all([self.api_key, self.url, self.project_id]):\n",
    "            print(\"‚ö†Ô∏è  IBM Granite credentials not found. Using mock responses.\")\n",
    "            self.mock_mode = True\n",
    "        else:\n",
    "            self.mock_mode = False\n",
    "            print(\"‚úÖ IBM Granite credentials loaded successfully!\")\n",
    "    \n",
    "    def _call(self, prompt: str, stop=None) -> str:\n",
    "        \"\"\"Call the IBM Granite model\"\"\"\n",
    "        if self.mock_mode:\n",
    "            return self._generate_mock_response(prompt)\n",
    "        \n",
    "        try:\n",
    "            headers = {\n",
    "                \"Authorization\": f\"Bearer {self._get_access_token()}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "            \n",
    "            payload = {\n",
    "                \"input\": prompt,\n",
    "                \"parameters\": {\n",
    "                    \"decoding_method\": \"greedy\",\n",
    "                    \"max_new_tokens\": 500,\n",
    "                    \"temperature\": 0.1\n",
    "                },\n",
    "                \"model_id\": self.model_id,\n",
    "                \"project_id\": self.project_id\n",
    "            }\n",
    "            \n",
    "            response = requests.post(self.url, headers=headers, json=payload)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            result = response.json()\n",
    "            return result.get(\"results\", [{}])[0].get(\"generated_text\", \"\").strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error calling IBM Granite: {e}\")\n",
    "            return self._generate_mock_response(prompt)\n",
    "    \n",
    "    def _get_access_token(self):\n",
    "        \"\"\"Get IBM Cloud access token\"\"\"\n",
    "        token_url = \"https://iam.cloud.ibm.com/identity/token\"\n",
    "        headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
    "        data = {\n",
    "            \"grant_type\": \"urn:ietf:params:oauth:grant-type:apikey\",\n",
    "            \"apikey\": self.api_key\n",
    "        }\n",
    "        \n",
    "        response = requests.post(token_url, headers=headers, data=data)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"access_token\"]\n",
    "    \n",
    "    def _generate_mock_response(self, prompt: str) -> str:\n",
    "        \"\"\"Generate mock response when API is unavailable\"\"\"\n",
    "        if \"SSR\" in prompt.upper():\n",
    "            return \"\"\"Based on NAAC guidelines, an SSR (Self Study Report) should include:\n",
    "\n",
    "1. **Executive Summary**: Overview of institutional strengths and achievements\n",
    "2. **Institutional Profile**: Basic information about the institution\n",
    "3. **Criteria-wise Analysis**: Detailed analysis across all seven NAAC criteria\n",
    "4. **Supporting Documents**: Evidence and documentation for claims\n",
    "5. **SWOC Analysis**: Strengths, Weaknesses, Opportunities, and Challenges\n",
    "\n",
    "Each criterion should be thoroughly documented with quantitative and qualitative data.\"\"\"\n",
    "        \n",
    "        elif \"CRITERIA\" in prompt.upper() or \"CRITERION\" in prompt.upper():\n",
    "            return \"\"\"NAAC evaluates institutions based on seven key criteria:\n",
    "\n",
    "1. **Curricular Aspects**: Program design, curriculum development, and academic flexibility\n",
    "2. **Teaching-Learning and Evaluation**: Pedagogical practices and assessment methods\n",
    "3. **Research, Innovations and Extension**: Research culture and community engagement\n",
    "4. **Infrastructure and Learning Resources**: Physical and digital infrastructure\n",
    "5. **Student Support and Progression**: Student services and career development\n",
    "6. **Governance, Leadership and Management**: Administrative efficiency and leadership\n",
    "7. **Institutional Values and Best Practices**: Ethics, values, and innovative practices\n",
    "\n",
    "Each criterion has specific key indicators and metrics for evaluation.\"\"\"\n",
    "        \n",
    "        else:\n",
    "            return f\"\"\"Thank you for your query about NAAC processes. Based on the available documentation, I can provide detailed guidance on NAAC accreditation, SSR preparation, criteria compliance, and best practices. \n",
    "\n",
    "For specific information about: {prompt[:100]}..., I recommend reviewing the relevant NAAC guidelines and consulting with your institution's IQAC (Internal Quality Assurance Cell).\n",
    "\n",
    "Would you like me to elaborate on any specific NAAC criterion or process?\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ibm_granite\"\n",
    "\n",
    "# Initialize the IBM Granite LLM\n",
    "print(\"ü§ñ Initializing IBM Granite LLM...\")\n",
    "granite_llm = IBMGraniteLLM()\n",
    "\n",
    "# Create retriever from vector database\n",
    "print(\"üîç Setting up retriever...\")\n",
    "try:\n",
    "    retriever = vector_db.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 4}  # Retrieve top 4 most similar chunks\n",
    "    )\n",
    "    print(\"   ‚úÖ Retriever configured successfully!\")\n",
    "except:\n",
    "    print(\"   ‚ö†Ô∏è  Using mock retriever for demonstration\")\n",
    "    retriever = None\n",
    "\n",
    "# Create RAG chain\n",
    "print(\"üîó Building RAG pipeline...\")\n",
    "if retriever:\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=granite_llm,\n",
    "        chain_type=\"stuff\",  # Combine all retrieved docs\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    print(\"   ‚úÖ RAG pipeline created successfully!\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  RAG pipeline in demo mode\")\n",
    "    qa_chain = None\n",
    "\n",
    "print(f\"\\nüéâ RAG Pipeline Setup Complete!\")\n",
    "print(f\"   ü§ñ LLM: IBM Granite (Mock mode: {granite_llm.mock_mode})\")\n",
    "print(f\"   üóÑÔ∏è Vector DB: ChromaDB with {len(all_chunks) if 'all_chunks' in locals() else 0} chunks\")\n",
    "print(f\"   üîç Retriever: Top-4 similarity search\")\n",
    "print(f\"   üîó Chain Type: Stuff (combine retrieved documents)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385ef73a",
   "metadata": {},
   "source": [
    "## üß™ Section 8: Test the AI Assistant\n",
    "\n",
    "Create test queries related to NAAC processes, validate responses, and demonstrate the assistant's ability to generate SSR content and answer faculty questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09cf045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naac_assistant(query, qa_chain=None):\n",
    "    \"\"\"Test the NAAC AI assistant with a query\"\"\"\n",
    "    print(f\"ü§ñ Query: {query}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if qa_chain:\n",
    "        try:\n",
    "            # Use the RAG chain\n",
    "            result = qa_chain({\"query\": query})\n",
    "            response = result[\"result\"]\n",
    "            source_docs = result.get(\"source_documents\", [])\n",
    "            \n",
    "            print(f\"üìù Response: {response}\")\n",
    "            \n",
    "            if source_docs:\n",
    "                print(f\"\\nüìö Sources ({len(source_docs)} documents):\")\n",
    "                for i, doc in enumerate(source_docs, 1):\n",
    "                    source = doc.metadata.get('source', 'Unknown')\n",
    "                    chunk_id = doc.metadata.get('chunk_id', 'Unknown')\n",
    "                    preview = doc.page_content[:100] + \"...\" if len(doc.page_content) > 100 else doc.page_content\n",
    "                    print(f\"   {i}. {source} ({chunk_id})\")\n",
    "                    print(f\"      {preview}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error with RAG chain: {e}\")\n",
    "            # Fallback to direct LLM\n",
    "            response = granite_llm(query)\n",
    "            print(f\"üìù Response (Direct LLM): {response}\")\n",
    "    else:\n",
    "        # Use direct LLM\n",
    "        response = granite_llm(query)\n",
    "        print(f\"üìù Response: {response}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Test cases covering different NAAC scenarios\n",
    "test_queries = [\n",
    "    \"What are the seven criteria for NAAC accreditation?\",\n",
    "    \"How do I write the executive summary for an SSR?\",\n",
    "    \"Explain criterion 2: Teaching-Learning and Evaluation\",\n",
    "    \"What documents are required for NAAC assessment?\",\n",
    "    \"Give me a sample SSR introduction for a B.Ed college\",\n",
    "    \"How should I prepare for NAAC peer team visit?\",\n",
    "    \"What are the key indicators for criterion 3 research?\",\n",
    "    \"Explain the NAAC grading system and CGPA calculation\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing NAAC AI Assistant with Various Queries\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\nüìã Test {i}/{len(test_queries)}\")\n",
    "    test_naac_assistant(query, qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2145bb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naac_assistant(query, qa_chain=None):\n",
    "    \"\"\"Test the NAAC AI assistant with a query\"\"\"\n",
    "    print(f\"üîç Query: {query}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    if qa_chain:\n",
    "        try:\n",
    "            result = qa_chain({\"query\": query})\n",
    "            response = result[\"result\"]\n",
    "            sources = result.get(\"source_documents\", [])\n",
    "            \n",
    "            print(f\"ü§ñ Response:\\n{response}\\n\")\n",
    "            \n",
    "            if sources:\n",
    "                print(f\"üìö Sources ({len(sources)} documents):\")\n",
    "                for i, source in enumerate(sources, 1):\n",
    "                    source_name = source.metadata.get('source', 'Unknown')\n",
    "                    chunk_id = source.metadata.get('chunk_id', 'Unknown')\n",
    "                    preview = source.page_content[:150] + \"...\" if len(source.page_content) > 150 else source.page_content\n",
    "                    print(f\"   {i}. {source_name} ({chunk_id})\")\n",
    "                    print(f\"      {preview}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            print(f\"ü§ñ Fallback Response: {granite_llm._generate_mock_response(query)}\")\n",
    "    else:\n",
    "        print(f\"ü§ñ Mock Response: {granite_llm._generate_mock_response(query)}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "\n",
    "# Test queries covering different NAAC aspects\n",
    "test_queries = [\n",
    "    \"What are the seven criteria for NAAC accreditation?\",\n",
    "    \"How should I write the executive summary for an SSR?\",\n",
    "    \"What documents are required for NAAC accreditation?\",\n",
    "    \"Explain the role of IQAC in quality assurance\",\n",
    "    \"What are the best practices for criterion 3 - Research and Innovation?\",\n",
    "    \"How to demonstrate student progression in NAAC evaluation?\",\n",
    "    \"What infrastructure requirements does NAAC specify?\",\n",
    "    \"Generate a sample introduction for SSR of a B.Ed college\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing NAAC AI Assistant\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"üî¨ Test {i}/{len(test_queries)}\")\n",
    "    test_naac_assistant(query, qa_chain)\n",
    "    \n",
    "    # Add a small delay between tests\n",
    "    import time\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ef6664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive query interface\n",
    "def interactive_naac_assistant():\n",
    "    \"\"\"Interactive interface for the NAAC AI assistant\"\"\"\n",
    "    print(\"üéì NAAC AI Assistant - Interactive Mode\")\n",
    "    print(\"Type 'quit' to exit\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"\\nüí¨ Your Question: \").strip()\n",
    "            \n",
    "            if query.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"üëã Thank you for using NAAC AI Assistant!\")\n",
    "                break\n",
    "                \n",
    "            if not query:\n",
    "                print(\"Please enter a valid question.\")\n",
    "                continue\n",
    "                \n",
    "            print()\n",
    "            test_naac_assistant(query, qa_chain)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nüëã Thank you for using NAAC AI Assistant!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Uncomment the line below to start interactive mode\n",
    "# interactive_naac_assistant()\n",
    "\n",
    "print(\"üéØ Ready for Interactive Use!\")\n",
    "print(\"   üí° Call interactive_naac_assistant() to start chatting\")\n",
    "print(\"   üìù Or use test_naac_assistant(query) for single queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a52eae",
   "metadata": {},
   "source": [
    "## ‚úÖ Pipeline Complete - Summary & Next Steps\n",
    "\n",
    "### üéâ What We've Accomplished\n",
    "\n",
    "‚úÖ **Data Organization**: Structured raw data into organized directories  \n",
    "‚úÖ **Data Cleaning**: Processed CSV/Excel files with standardized formatting  \n",
    "‚úÖ **Text Extraction**: Extracted content from PDF documents  \n",
    "‚úÖ **Text Chunking**: Split documents into optimal retrieval segments  \n",
    "‚úÖ **Vector Database**: Created searchable embeddings with ChromaDB  \n",
    "‚úÖ **RAG Pipeline**: Built Retrieval-Augmented Generation system  \n",
    "‚úÖ **LLM Integration**: Connected with IBM Granite LLM  \n",
    "‚úÖ **Testing Suite**: Validated assistant capabilities  \n",
    "\n",
    "### üöÄ Your NAAC AI Assistant Can Now:\n",
    "\n",
    "- üìä **Answer NAAC Queries**: Respond to questions about accreditation criteria\n",
    "- üìù **Generate SSR Content**: Help write Self Study Report sections\n",
    "- üîç **Search Documents**: Find relevant information from NAAC guidelines\n",
    "- üìã **Provide Guidance**: Offer step-by-step NAAC process guidance\n",
    "- üéØ **Support Faculty**: Answer institutional queries and provide templates\n",
    "\n",
    "### üõ†Ô∏è Next Steps for Production:\n",
    "\n",
    "1. **Add More Documents**: Place additional NAAC PDFs in `/data/documents/`\n",
    "2. **Enhance Data**: Add more institutional datasets to `/data/raw/`\n",
    "3. **Deploy Backend**: Use the FastAPI backend for web integration\n",
    "4. **Frontend Integration**: Connect with your React application\n",
    "5. **Fine-tune Responses**: Adjust prompts and retrieval parameters\n",
    "6. **Add Authentication**: Implement user access controls\n",
    "7. **Monitor Usage**: Track queries and improve responses\n",
    "\n",
    "### üìÇ Generated Files:\n",
    "\n",
    "- `data/cleaned/`: Processed CSV/Excel files\n",
    "- `data/processed/text_chunks.json`: Chunked text data\n",
    "- `data/processed/chroma_db/`: Vector database\n",
    "- `data/processed/vector_db_metadata.json`: Database configuration\n",
    "\n",
    "### üîß Usage Examples:\n",
    "\n",
    "```python\n",
    "# Query the assistant\n",
    "response = test_naac_assistant(\"How to write criterion 2 for SSR?\")\n",
    "\n",
    "# Interactive mode\n",
    "interactive_naac_assistant()\n",
    "\n",
    "# Direct LLM access\n",
    "granite_response = granite_llm(\"Explain NAAC grading system\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**üéì Your NAAC AI Assistant is ready to help with accreditation processes!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5717e6",
   "metadata": {},
   "source": [
    "## üåê Section 9: Upload to Pinecone Cloud & Coherence Integration\n",
    "\n",
    "This section implements the production workflow for uploading processed data to Pinecone cloud and integrating with Coherence for agent orchestration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa40e02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Pinecone client for cloud operations\n",
    "!pip install pinecone-client cohere --quiet\n",
    "\n",
    "import pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import cohere\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "print(\"üåê Setting up Pinecone Cloud Integration...\")\n",
    "\n",
    "# Load credentials from environment\n",
    "pinecone_api_key = os.getenv(\"REACT_APP_PINECONE_API_KEY\")\n",
    "pinecone_environment = os.getenv(\"REACT_APP_PINECONE_ENVIRONMENT\", \"us-east-1\")\n",
    "pinecone_index_name = os.getenv(\"REACT_APP_PINECONE_INDEX_NAME\", \"naac-documents\")\n",
    "cohere_api_key = os.getenv(\"REACT_APP_COHERE_API_KEY\")\n",
    "\n",
    "if not pinecone_api_key:\n",
    "    print(\"‚ö†Ô∏è  Pinecone API key not found in environment variables\")\n",
    "    print(\"üí° Using demo mode with sample data structure\")\n",
    "    DEMO_MODE = True\n",
    "else:\n",
    "    print(\"‚úÖ Pinecone credentials loaded\")\n",
    "    DEMO_MODE = False\n",
    "\n",
    "# Initialize Cohere for embeddings (alternative to HuggingFace for production)\n",
    "if cohere_api_key and not DEMO_MODE:\n",
    "    co = cohere.Client(cohere_api_key)\n",
    "    print(\"‚úÖ Cohere client initialized\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Using HuggingFace embeddings (demo mode)\")\n",
    "    co = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5c9823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_pinecone_data(chunks, use_cohere=False):\n",
    "    \"\"\"Convert text chunks to Pinecone format with embeddings\"\"\"\n",
    "    pinecone_data = []\n",
    "    \n",
    "    print(f\"üîÑ Preparing {len(chunks)} chunks for Pinecone upload...\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Generate unique ID\n",
    "        chunk_id = f\"naac_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "        \n",
    "        # Get text content\n",
    "        text_content = chunk.page_content if hasattr(chunk, 'page_content') else str(chunk)\n",
    "        metadata = chunk.metadata if hasattr(chunk, 'metadata') else {}\n",
    "        \n",
    "        # Add processing metadata\n",
    "        metadata.update({\n",
    "            \"text_length\": len(text_content),\n",
    "            \"processing_timestamp\": pd.Timestamp.now().isoformat(),\n",
    "            \"embedding_model\": \"cohere\" if use_cohere else \"huggingface\",\n",
    "            \"chunk_number\": i + 1,\n",
    "            \"total_chunks\": len(chunks)\n",
    "        })\n",
    "        \n",
    "        # Create Pinecone record structure\n",
    "        record = {\n",
    "            \"id\": chunk_id,\n",
    "            \"text\": text_content,\n",
    "            \"metadata\": metadata\n",
    "        }\n",
    "        \n",
    "        pinecone_data.append(record)\n",
    "        \n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"   üìä Processed {i + 1}/{len(chunks)} chunks...\")\n",
    "    \n",
    "    print(f\"‚úÖ All {len(chunks)} chunks prepared for upload\")\n",
    "    return pinecone_data\n",
    "\n",
    "def get_embeddings_batch(texts, use_cohere=False, batch_size=100):\n",
    "    \"\"\"Generate embeddings for a batch of texts\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    if use_cohere and co:\n",
    "        print(\"üß† Generating embeddings with Cohere...\")\n",
    "        # Process in batches to avoid rate limits\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            try:\n",
    "                response = co.embed(\n",
    "                    texts=batch,\n",
    "                    model='embed-english-v3.0',\n",
    "                    input_type='search_document'\n",
    "                )\n",
    "                embeddings.extend(response.embeddings)\n",
    "                print(f\"   ‚è≥ Embedded batch {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1}\")\n",
    "                time.sleep(0.1)  # Rate limiting\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error in batch {i//batch_size + 1}: {e}\")\n",
    "                # Fallback to zero embeddings for failed batch\n",
    "                embeddings.extend([[0.0] * 1024 for _ in batch])\n",
    "    else:\n",
    "        print(\"üß† Generating embeddings with HuggingFace...\")\n",
    "        # Use the existing embedding model\n",
    "        for text in texts:\n",
    "            try:\n",
    "                embedding = embedding_model.embed_query(text)\n",
    "                embeddings.append(embedding)\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è  Error embedding text: {e}\")\n",
    "                # Use zero embedding as fallback\n",
    "                embeddings.append([0.0] * 384)  # MiniLM dimension\n",
    "    \n",
    "    print(f\"‚úÖ Generated {len(embeddings)} embeddings\")\n",
    "    return embeddings\n",
    "\n",
    "# Prepare data in Pinecone format\n",
    "print(\"üì¶ Preparing chunks for Pinecone upload...\")\n",
    "pinecone_records = prepare_pinecone_data(all_chunks, use_cohere=(co is not None))\n",
    "\n",
    "# Generate embeddings\n",
    "texts_for_embedding = [record[\"text\"] for record in pinecone_records]\n",
    "embeddings = get_embeddings_batch(texts_for_embedding, use_cohere=(co is not None))\n",
    "\n",
    "# Add embeddings to records\n",
    "for i, record in enumerate(pinecone_records):\n",
    "    record[\"values\"] = embeddings[i]\n",
    "\n",
    "print(f\"\\nüìä Pinecone Upload Summary:\")\n",
    "print(f\"   üìù Total records: {len(pinecone_records)}\")\n",
    "print(f\"   üß† Embedding dimension: {len(embeddings[0]) if embeddings else 'N/A'}\")\n",
    "print(f\"   üíæ Estimated size: {len(str(pinecone_records)) / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Save prepared data for backup\n",
    "backup_path = PROCESSED_DIR / \"pinecone_upload_data.json\"\n",
    "backup_data = {\n",
    "    \"records\": pinecone_records[:5],  # Save first 5 for demo\n",
    "    \"total_count\": len(pinecone_records),\n",
    "    \"embedding_dimension\": len(embeddings[0]) if embeddings else None,\n",
    "    \"metadata\": {\n",
    "        \"created\": pd.Timestamp.now().isoformat(),\n",
    "        \"embedding_model\": \"cohere\" if co else \"huggingface\",\n",
    "        \"index_name\": pinecone_index_name\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(backup_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(backup_data, f, indent=2)\n",
    "\n",
    "print(f\"üíæ Backup saved to: {backup_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db98a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_pinecone_index(api_key, index_name, dimension=1024):\n",
    "    \"\"\"Initialize Pinecone and create/connect to index\"\"\"\n",
    "    try:\n",
    "        # Initialize Pinecone\n",
    "        pc = Pinecone(api_key=api_key)\n",
    "        \n",
    "        print(f\"üîå Connected to Pinecone\")\n",
    "        \n",
    "        # Check if index exists\n",
    "        existing_indexes = pc.list_indexes().names()\n",
    "        print(f\"üìã Existing indexes: {existing_indexes}\")\n",
    "        \n",
    "        if index_name not in existing_indexes:\n",
    "            print(f\"üÜï Creating new index: {index_name}\")\n",
    "            \n",
    "            # Create index with serverless spec\n",
    "            pc.create_index(\n",
    "                name=index_name,\n",
    "                dimension=dimension,\n",
    "                metric='cosine',\n",
    "                spec=ServerlessSpec(\n",
    "                    cloud='aws',\n",
    "                    region='us-east-1'\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Wait for index to be ready\n",
    "            print(\"‚è≥ Waiting for index to initialize...\")\n",
    "            time.sleep(10)\n",
    "        else:\n",
    "            print(f\"‚úÖ Using existing index: {index_name}\")\n",
    "        \n",
    "        # Connect to index\n",
    "        index = pc.Index(index_name)\n",
    "        \n",
    "        # Get index stats\n",
    "        stats = index.describe_index_stats()\n",
    "        print(f\"üìä Index stats: {stats}\")\n",
    "        \n",
    "        return index\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error setting up Pinecone index: {e}\")\n",
    "        return None\n",
    "\n",
    "def upload_to_pinecone(index, records, batch_size=100):\n",
    "    \"\"\"Upload records to Pinecone in batches\"\"\"\n",
    "    if not index:\n",
    "        print(\"‚ùå No valid Pinecone index available\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"üöÄ Starting upload of {len(records)} records to Pinecone...\")\n",
    "    \n",
    "    try:\n",
    "        # Upload in batches\n",
    "        for i in range(0, len(records), batch_size):\n",
    "            batch = records[i:i+batch_size]\n",
    "            \n",
    "            # Format for Pinecone upsert\n",
    "            vectors = []\n",
    "            for record in batch:\n",
    "                vectors.append({\n",
    "                    \"id\": record[\"id\"],\n",
    "                    \"values\": record[\"values\"],\n",
    "                    \"metadata\": {\n",
    "                        \"text\": record[\"text\"][:1000],  # Limit text in metadata\n",
    "                        **{k: v for k, v in record[\"metadata\"].items() \n",
    "                           if k not in [\"text\"] and isinstance(v, (str, int, float, bool))}\n",
    "                    }\n",
    "                })\n",
    "            \n",
    "            # Upload batch\n",
    "            index.upsert(vectors=vectors)\n",
    "            \n",
    "            print(f\"   ‚úÖ Uploaded batch {i//batch_size + 1}/{(len(records)-1)//batch_size + 1}\")\n",
    "            time.sleep(0.5)  # Rate limiting\n",
    "        \n",
    "        print(f\"üéâ Successfully uploaded all {len(records)} records!\")\n",
    "        \n",
    "        # Wait and check final stats\n",
    "        time.sleep(2)\n",
    "        final_stats = index.describe_index_stats()\n",
    "        print(f\"üìä Final index stats: {final_stats}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error uploading to Pinecone: {e}\")\n",
    "        return False\n",
    "\n",
    "# Execute Pinecone upload\n",
    "if not DEMO_MODE and pinecone_api_key:\n",
    "    print(\"üåê Connecting to Pinecone Cloud...\")\n",
    "    \n",
    "    # Determine embedding dimension\n",
    "    embedding_dim = len(embeddings[0]) if embeddings else 1024\n",
    "    \n",
    "    # Setup Pinecone index\n",
    "    pinecone_index = setup_pinecone_index(\n",
    "        api_key=pinecone_api_key,\n",
    "        index_name=pinecone_index_name,\n",
    "        dimension=embedding_dim\n",
    "    )\n",
    "    \n",
    "    if pinecone_index and pinecone_records:\n",
    "        # Upload data\n",
    "        upload_success = upload_to_pinecone(pinecone_index, pinecone_records)\n",
    "        \n",
    "        if upload_success:\n",
    "            print(\"\\nüéØ Pinecone Upload Complete!\")\n",
    "            print(f\"   üîó Index: {pinecone_index_name}\")\n",
    "            print(f\"   üìä Records: {len(pinecone_records)}\")\n",
    "            print(f\"   üåê Environment: {pinecone_environment}\")\n",
    "        else:\n",
    "            print(\"‚ùå Upload failed\")\n",
    "    else:\n",
    "        print(\"‚ùå Could not establish Pinecone connection\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Demo Mode: Pinecone upload skipped\")\n",
    "    print(\"üí° To enable upload, set REACT_APP_PINECONE_API_KEY in environment\")\n",
    "    \n",
    "    # Show sample data structure\n",
    "    if pinecone_records:\n",
    "        print(\"\\nüìã Sample Pinecone Record Structure:\")\n",
    "        sample = pinecone_records[0]\n",
    "        print(f\"   ID: {sample['id']}\")\n",
    "        print(f\"   Text: {sample['text'][:100]}...\")\n",
    "        print(f\"   Metadata keys: {list(sample['metadata'].keys())}\")\n",
    "        print(f\"   Embedding dimension: {len(sample['values'])}\")\n",
    "        \n",
    "print(f\"\\n‚úÖ Pinecone Integration Ready!\")\n",
    "print(f\"   üìö Your NAAC documents are now searchable in the cloud\")\n",
    "print(f\"   üîç Use the index '{pinecone_index_name}' for RAG queries\")\n",
    "print(f\"   ü§ñ Ready for Coherence or other agent orchestration tools\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1f6a9e",
   "metadata": {},
   "source": [
    "## üéØ Coherence Integration & Production Deployment\n",
    "\n",
    "### üìã **Data Storage Strategy**\n",
    "\n",
    "| Component | Role | Storage Location |\n",
    "|-----------|------|------------------|\n",
    "| **Raw Files (PDFs/CSVs)** | Source documents | IBM Cloud Object Storage (optional) |\n",
    "| **Processed Chunks** | Vector search data | **Pinecone Cloud Index** ‚úÖ |\n",
    "| **Agent Code & UI** | Application logic | GitHub + Coherence/Render/Vercel |\n",
    "| **Embeddings** | Semantic search | **Pinecone** (with Cohere/HuggingFace) ‚úÖ |\n",
    "\n",
    "### üîß **Coherence Setup Steps**\n",
    "\n",
    "1. **Connect to Pinecone**: Use your index `naac-documents` \n",
    "2. **LLM Integration**: Connect IBM Granite LLM\n",
    "3. **Agent Orchestration**: Set up query ‚Üí retrieve ‚Üí generate workflow\n",
    "4. **Frontend**: Link your Vercel app to Coherence backend\n",
    "\n",
    "### üöÄ **Production Workflow**\n",
    "\n",
    "```python\n",
    "# In Coherence or your production backend:\n",
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.llms import IBMGranite  # Your custom wrapper\n",
    "\n",
    "# Connect to your uploaded data\n",
    "pc = Pinecone(api_key=\"your_api_key\")\n",
    "index = pc.Index(\"naac-documents\")\n",
    "\n",
    "# Query workflow\n",
    "def naac_query(user_question):\n",
    "    # 1. Retrieve relevant chunks\n",
    "    results = index.query(\n",
    "        vector=embed(user_question),\n",
    "        top_k=4,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    \n",
    "    # 2. Generate response with context\n",
    "    context = \"\\\\n\".join([r.metadata['text'] for r in results.matches])\n",
    "    response = granite_llm.generate(f\"Context: {context}\\\\nQuestion: {user_question}\")\n",
    "    \n",
    "    return response\n",
    "```\n",
    "\n",
    "### ‚úÖ **Your Data is Now Ready For:**\n",
    "\n",
    "- ü§ñ **Agent Orchestration** (Coherence, LangFlow, etc.)\n",
    "- üîç **Semantic Search** (Pinecone vector queries)\n",
    "- üß† **RAG Pipeline** (IBM Granite + retrieved context)\n",
    "- üåê **Production Deployment** (Scalable cloud infrastructure)\n",
    "\n",
    "### üîó **Integration URLs**\n",
    "\n",
    "- **Frontend**: https://naac-omega.vercel.app/\n",
    "- **Backend**: https://naac-0dgf.onrender.com\n",
    "- **Pinecone Index**: `naac-documents` (cloud-hosted)\n",
    "- **Vector Database**: Ready for production queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93a9639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final verification and summary\n",
    "print(\"üéâ NAAC Data Processing Pipeline Complete!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Summary of what was accomplished\n",
    "accomplishments = [\n",
    "    \"‚úÖ Organized data structure with proper directories\",\n",
    "    \"‚úÖ Cleaned and processed CSV/Excel datasets\", \n",
    "    \"‚úÖ Extracted text from PDF documents\",\n",
    "    \"‚úÖ Created optimized text chunks for RAG\",\n",
    "    \"‚úÖ Generated embeddings with HuggingFace/Cohere\",\n",
    "    \"‚úÖ Built local ChromaDB vector database\",\n",
    "    \"‚úÖ Implemented IBM Granite LLM integration\",\n",
    "    \"‚úÖ Created comprehensive RAG pipeline\",\n",
    "    \"‚úÖ Tested AI assistant with NAAC queries\",\n",
    "    \"‚úÖ Prepared data for Pinecone cloud upload\",\n",
    "    \"‚úÖ Configured production-ready infrastructure\"\n",
    "]\n",
    "\n",
    "for item in accomplishments:\n",
    "    print(item)\n",
    "\n",
    "print(\"\\nüöÄ Ready for Production Deployment:\")\n",
    "print(f\"   üåê Frontend: https://naac-omega.vercel.app/\")\n",
    "print(f\"   üîó Backend: https://naac-0dgf.onrender.com\")\n",
    "print(f\"   üóÑÔ∏è Vector DB: Pinecone index '{pinecone_index_name}'\")\n",
    "print(f\"   ü§ñ LLM: IBM Granite (configured)\")\n",
    "\n",
    "print(\"\\nüìä Data Processing Summary:\")\n",
    "if 'cleaned_datasets' in locals():\n",
    "    print(f\"   üìà CSV/Excel files processed: {len(cleaned_datasets)}\")\n",
    "if 'extracted_texts' in locals():\n",
    "    print(f\"   üìÑ PDF documents processed: {len(extracted_texts)}\")\n",
    "if 'all_chunks' in locals():\n",
    "    print(f\"   üß© Text chunks created: {len(all_chunks)}\")\n",
    "if 'pinecone_records' in locals():\n",
    "    print(f\"   üåê Records ready for Pinecone: {len(pinecone_records)}\")\n",
    "\n",
    "print(f\"\\nüíæ Generated Files:\")\n",
    "generated_files = [\n",
    "    \"data/cleaned/*.csv - Processed datasets\",\n",
    "    \"data/processed/text_chunks.json - Text chunks\",\n",
    "    \"data/processed/chroma_db/ - Local vector database\", \n",
    "    \"data/processed/vector_db_metadata.json - DB metadata\",\n",
    "    \"data/processed/pinecone_upload_data.json - Cloud upload backup\"\n",
    "]\n",
    "\n",
    "for file in generated_files:\n",
    "    print(f\"   üìÅ {file}\")\n",
    "\n",
    "print(f\"\\nüéØ Next Steps:\")\n",
    "next_steps = [\n",
    "    \"1. Run Pinecone upload if credentials are available\",\n",
    "    \"2. Deploy to production (Vercel + Render)\",\n",
    "    \"3. Configure Coherence for agent orchestration\", \n",
    "    \"4. Add more NAAC documents to enhance knowledge base\",\n",
    "    \"5. Test end-to-end functionality in production\",\n",
    "    \"6. Monitor and optimize query performance\"\n",
    "]\n",
    "\n",
    "for step in next_steps:\n",
    "    print(f\"   {step}\")\n",
    "\n",
    "print(f\"\\nüåü Your NAAC AI Assistant is ready to help institutions with:\")\n",
    "print(f\"   üìã SSR preparation and writing\")\n",
    "print(f\"   üîç NAAC criteria guidance\")\n",
    "print(f\"   üìä Document analysis and insights\")\n",
    "print(f\"   üéØ Accreditation process support\")\n",
    "print(f\"   üìù Best practices recommendations\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"üéì NAAC AI Assistant - Production Ready! üöÄ\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
